{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IRDM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "UbC9BTOTbK70",
        "s_-JWZR8bfDj",
        "xih9PozpcY9_"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "JMDP09mXvOQv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You can download all data from [All data](https://drive.google.com/open?id=1Xxp055WzGNsF_J8lxR2_5pehlnb0Igco). The following is my code"
      ]
    },
    {
      "metadata": {
        "id": "6eGzS9wNuYWP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "EGTUwmzLfoG_",
        "colab_type": "code",
        "outputId": "4a962afa-be41-49cb-f486-9bd0b8ecabc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/drive',force_remount=True)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ObGHFM_3oUbk",
        "colab_type": "code",
        "outputId": "227a60c7-07e8-43c5-f7a4-6903df446112",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "cell_type": "code",
      "source": [
        "import json\n",
        "import nltk\n",
        "import gensim\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "import gensim.corpora as corpora\n",
        "from matplotlib import pyplot as plt\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "import string\n",
        "import numpy as np\n",
        "from numpy import random\n",
        "import pandas as pd\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
            "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "ktiOGjE_oQHs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#0. Load Data"
      ]
    },
    {
      "metadata": {
        "id": "vn60JeOOoP1e",
        "colab_type": "code",
        "outputId": "e5b01569-9df7-4f6d-f36f-8be36c73193e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "directory = os.listdir('./wiki-pages/wiki-pages/')\n",
        "print(len(directory))\n",
        "file_list=[]\n",
        "for file in directory:\n",
        "  file_list.append(file)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "109\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bF3u7XdyqMcp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Remove stopword word & punctuation"
      ]
    },
    {
      "metadata": {
        "id": "M8EdMXUwo5Zm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer  \n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "def get_tokens(text):\n",
        "    lower = text.lower()\n",
        "    remove_punctuation_map = dict((ord(char), None) for char in string.punctuation)\n",
        "    no_punctuation = lower.translate(remove_punctuation_map)\n",
        "    tokens = nltk.word_tokenize(no_punctuation)\n",
        "    tokens=[word for word in tokens if word not in ['rrb','lrb']]\n",
        "    return [word for word in tokens if word not in stop_words]\n",
        "  \n",
        "def lemmatized_tokens(tokens, lemmatizer):\n",
        "    lemmatized = []\n",
        "    for item in tokens:\n",
        "        lemmatized.append(lemmatizer.lemmatize(item))\n",
        "    return lemmatized"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MMlsO8LCux6_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Save and Load data to dictory"
      ]
    },
    {
      "metadata": {
        "id": "rDMO8AuDu3Dt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def save_dict(filename,dic):\n",
        "  np.save(filename, dic) \n",
        "\n",
        "def load_dict(filename):\n",
        "  read_dictionary = np.load(filename,allow_pickle=True).item()\n",
        "  return read_dictionary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9rD3se-9IWZT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#1. Task1: Text Statistics"
      ]
    },
    {
      "metadata": {
        "id": "NKhmi8YkIU-C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def traverse_directory_all(list_dir):\n",
        "  dict1={}\n",
        "  count=1\n",
        "  for file in list_dir:\n",
        "      print(\"file name is \"+ str(file))\n",
        "      test=open('/drive/My Drive/IRDM/wiki-pages/wiki-pages/'+file)  \n",
        "      for line in test:\n",
        "        json_obj=json.loads(line)\n",
        "        line=get_tokens(json_obj['text'])\n",
        "        for term in line:\n",
        "          if term in dict1:\n",
        "            dict1[term]=dict1[term]+1\n",
        "          else:\n",
        "            dict1[term]=1\n",
        "      print(count)\n",
        "      count+=1\n",
        "      test.close()\n",
        "  save_dict('/drive/My Drive/IRDM/task1/task1_dict.npy',dict1)\n",
        "  return dict1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h4wVFMcKvIiQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "traverse_directory_all(file_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DkwznwioMZk6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dict1=load_dict('/drive/My Drive/IRDM/task1/task1_dict.npy')\n",
        "ranked=sorted(dict1.items(),key=lambda item:item[1],reverse=True)\n",
        "x=[]\n",
        "y=[]\n",
        "\n",
        "total=np.int64(0)\n",
        "total=total+np.int64(sum(dict1.values()))\n",
        "statis=[]\n",
        "\n",
        "for i in range(len(ranked)):\n",
        "  x.append(i+1)\n",
        "  y.append(ranked[i][1])\n",
        "  \n",
        "for i in range(30):\n",
        "  statis.append([ranked[i][0],ranked[i][1],i+1,float(ranked[i][1]/total),(i+1)*float(ranked[i][1]/total)])\n",
        "\n",
        "print(statis)\n",
        "plt.plot(x,y)\n",
        "plt.xlabel('ranked')\n",
        "plt.ylabel('frequencey')\n",
        "plt.xscale('log')\n",
        "plt.yscale('log')\n",
        "plt.savefig('/drive/My Drive/IRDM/zip.jpg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yQJWkl5KZ-Hm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 2. Task2: TF-IDF\n"
      ]
    },
    {
      "metadata": {
        "id": "XWL2ul19TPwq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##TF-IDF"
      ]
    },
    {
      "metadata": {
        "id": "yvnLB1ljpa5m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "for file in file_list:\n",
        "    file=file.split(\".\")[0]\n",
        "    print(file)\n",
        "    test_file=open(\"/drive/My Drive/IRDM/wiki-pages/wiki-pages/\"+str(file)+\".jsonl\")\n",
        "    test_output=open(\"/drive/My Drive/IRDM/task2/frequency/\"+str(file)+\".txt\",'w')\n",
        "    output=open(\"/drive/My Drive/IRDM/task2/tokens/\"+str(file)+\".txt\",'w')\n",
        "    for line in test_file:\n",
        "      json_obj=json.loads(line)\n",
        "      tokens=get_tokens(json_obj['text'])\n",
        "      res_id=json_obj['id']\n",
        "      lemmatizer=WordNetLemmatizer()  \n",
        "      res_tokens=lemmatized_tokens(tokens, lemmatizer)\n",
        "      count = Counter(res_tokens)\n",
        "      temp=count.items()\n",
        "      test_output.writelines(\"id:\"+str(res_id)+\"#tokens:\")\n",
        "      for key,value in temp:\n",
        "        test_output.writelines(str(key)+\":\"+str(value)+',')\n",
        "      test_output.writelines(\"\\n\")\n",
        "      output.writelines(\"id:\"+str(res_id)+\";tokens:\"+str(res_tokens)+'\\n')\n",
        "\n",
        "    test_output.close()\n",
        "    output.close()\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qpX1OzwIJniW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_count():\n",
        "  train=open(\"/drive/My Drive/IRDM/train_temp.jsonl\")\n",
        "  output=open(\"/drive/My Drive/IRDM/task2/train.txt\",'w')\n",
        "  for line in train:\n",
        "    json_obj=json.loads(line)\n",
        "    tokens=get_tokens(json_obj['claim'])\n",
        "    res_id=json_obj['id']\n",
        "    lemmatizer=WordNetLemmatizer()  \n",
        "    res_tokens=lemmatized_tokens(tokens, lemmatizer)\n",
        "    count = Counter(res_tokens)\n",
        "    temp=count.items()\n",
        "    output.writelines(\"id:\"+str(res_id)+\"#tokens:\")\n",
        "    for key,value in temp:\n",
        "      output.writelines(str(key)+\":\"+str(value)+',')\n",
        "    output.writelines(\"\\n\")\n",
        "    \n",
        "  output.close()\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b8kkYPO87Pr8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import math\n",
        "\n",
        "directory = os.listdir('/drive/My Drive/IRDM/task2/frequency')\n",
        "\n",
        "\n",
        "def tf_claim():\n",
        "    claim=open('/drive/My Drive/IRDM/task2/train.txt','r')\n",
        "    for i in claim.readlines():\n",
        "        dict_temp={}\n",
        "        id,tokens=i.strip().split(\"#\")\n",
        "        print(id)\n",
        "        tokens=tokens[7:len(tokens)-1]\n",
        "        token_list=tokens.split(\",\")\n",
        "        for i1 in token_list:\n",
        "            word,fre=i1.split(\":\")\n",
        "            dict_temp[word]=int(fre)\n",
        "\n",
        "        for i2 in dict_temp.keys():\n",
        "            tf=dict_temp[i2]/sum(dict_temp.values())\n",
        "            print(i2+\":\"+str(tf))\n",
        "\n",
        "def claim_text():\n",
        "    claim=open('/drive/My Drive/IRDM/task2/train.txt','r')\n",
        "    dict={}\n",
        "    for i in claim.readlines():\n",
        "        text_list=[]\n",
        "        id,tokens=i.strip().split(\"#\")\n",
        "        id=id.split(\":\")[1]\n",
        "        tokens=tokens[7:len(tokens)-1]\n",
        "        token_list=tokens.split(\",\")\n",
        "        for i1 in token_list:\n",
        "            word,fre=i1.split(\":\")\n",
        "            text_list.append(word)\n",
        "        dict[str(id)]=text_list\n",
        "\n",
        "    return dict\n",
        "\n",
        "def tf_idf_doc(claim_id,text):\n",
        "    dict_text_num={}\n",
        "    dict_id_tf={}\n",
        "    dict_text_idf={}\n",
        "    for i2 in text:\n",
        "        dict_text_num[i2] = 0\n",
        "\n",
        "    for file_name in file_list:\n",
        "        print(file_name)\n",
        "        file=open(\"/drive/My Drive/IRDM/task2/frequency/\"+file_name,'r')\n",
        "        for i in file.readlines():\n",
        "            dict_temp={}\n",
        "            id,tokens=i.strip().split(\"#\")\n",
        "            id=id.split(\":\")[1]\n",
        "            if(tokens!='tokens:'):\n",
        "                tokens=tokens[7:len(tokens)-1]\n",
        "                token_list=tokens.split(\",\")\n",
        "                for i1 in token_list:\n",
        "                    word,fre=i1.split(\":\")\n",
        "                    dict_temp[word]=int(fre)\n",
        "            tf_list = []\n",
        "            for i2 in text:\n",
        "                if(i2 in dict_temp):\n",
        "                    #print(i2)\n",
        "                    dict_text_num[i2]+=1\n",
        "                    tf=dict_temp[i2]/sum(dict_temp.values())\n",
        "                    tf_list.append(float(tf))\n",
        "                    #print(tf)\n",
        "                else:\n",
        "                    tf=0\n",
        "                    tf_list.append(float(tf))\n",
        "            dict_id_tf[id]=tf_list\n",
        "\n",
        "    output1=open(\"/drive/My Drive/IRDM/task2/\"+str(claim_id)+\"_id_tf.txt\",'w')\n",
        "    dict_id_tf_new={}\n",
        "    for text3,number3 in dict_id_tf.items():\n",
        "        for i4 in number3:\n",
        "            if(i4!=0):\n",
        "                output1.writelines(text3+\":\"+str(number3)+\"\\n\")\n",
        "                dict_id_tf_new[text3]=number3\n",
        "                break\n",
        "    output1.close()\n",
        "\n",
        "    output2=open(\"/drive/My Drive/IRDM/task2/\"+str(claim_id)+\"_idf.txt\",'w')\n",
        "    for text1,number in dict_text_num.items():\n",
        "        idf=math.log(5395867)/(1+number)\n",
        "        dict_text_idf[text1]=float(idf)\n",
        "        output2.writelines(text1+\":\"+str(idf)+'\\n')\n",
        "\n",
        "    idf_vector = []\n",
        "    for text2,docu_idf in dict_text_idf.items():\n",
        "        idf_vector.append(docu_idf)\n",
        "\n",
        "    dict_id_tf_idf={}\n",
        "    for text4,tf in dict_id_tf_new.items():\n",
        "        tf_idf_vector=[]\n",
        "        for i5 in range(len(idf_vector)):\n",
        "            tf_idf=float(tf[i5])*float(idf_vector[i5])\n",
        "            tf_idf_vector.append(float(tf_idf))\n",
        "        dict_id_tf_idf[text4]=tf_idf_vector\n",
        "\n",
        "    output3=open(\"/drive/My Drive/IRDM/task2/\"+str(claim_id)+\"_tf_idf.txt\",'w')\n",
        "    for text5,tf_idf_now in dict_id_tf_idf.items():\n",
        "        output3.writelines(\"id:\"+text5+\"#\"+\"tf-idf:\"+str(tf_idf_now)+'\\n')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YaO7GiVmnjUD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "text=claim_text()\n",
        "for key,value in text.items():\n",
        "    tf_idf_doc(key,value)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OZPNP2qRTNe7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Cosine Similarity"
      ]
    },
    {
      "metadata": {
        "id": "ECYJc5QheP5K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def top_five_cos_sim():\n",
        "    claim_tf_idf=open(\"/drive/My Drive/IRDM/task2/claim-tf-idf/claim_tf-idf.txt\",'r')\n",
        "    claim_tf_idf_dict={}\n",
        "    for i in claim_tf_idf.readlines():\n",
        "        id_1,vec=i.strip().split(\";\")\n",
        "        claim_id=id_1.split(\":\")[1]\n",
        "        vec=vec[1:len(vec)-1].split(\",\")\n",
        "        tf_idf_vec=[]\n",
        "        for i1 in vec:\n",
        "            tf_idf_vec.append(float(i1))\n",
        "        claim_tf_idf_dict[claim_id]=tf_idf_vec\n",
        "\n",
        "    directory = os.listdir('/drive/My Drive/IRDM/doc-tf-idf/')\n",
        "    file_list = []\n",
        "    for file in directory:\n",
        "        file_list.append(file)\n",
        "\n",
        "    for i2 in file_list:\n",
        "        id_claim=i2.split(\"_\",1)[0]\n",
        "        print(id_claim)\n",
        "        file_temp=open(\"/drive/My Drive/IRDM/doc-tf-idf/\"+i2,'r')\n",
        "        claim_tf_idf_vec=pd.Series(claim_tf_idf_dict[id_claim])\n",
        "        cos_dict={}\n",
        "        for i3 in file_temp.readlines():\n",
        "            id_2,vec=i3.split(\"#\")\n",
        "            id_doc=id_2.split(\":\",1)[1]\n",
        "            vec=vec.split(':',1)[1]\n",
        "            vec=vec[1:len(vec)-2]\n",
        "\n",
        "            doc_tf_idf=[]\n",
        "            for i4 in vec.split(\",\"):\n",
        "                doc_tf_idf.append(float(i4))\n",
        "            doc_tf_id_vec=pd.Series(doc_tf_idf)\n",
        "            cos_sim_value=cos_sim(doc_tf_id_vec,claim_tf_idf_vec)\n",
        "            cos_dict[id_doc]=cos_sim_value\n",
        "\n",
        "        top_five=order_dict_N(cos_dict,5)\n",
        "        output=open('/drive/My Drive/IRDM/'+id_claim+\"_cos_sim_value.txt\",'w')\n",
        "        for i5 in top_five:\n",
        "            output.writelines(str(i5)+\"\\n\")\n",
        "\n",
        "        output.close()\n",
        "        print(id_claim+\" finish!!\")\n",
        "\n",
        "def order_dict_N(dict, N):\n",
        "    b = sorted([(k, v) for k, v in dict.items()], reverse=True)\n",
        "    e = set()\n",
        "    for i in b:\n",
        "        e.add(i[1])\n",
        "    result = []\n",
        "    for i in sorted(e, reverse=True)[:N]:\n",
        "        for j in b:\n",
        "           if j[1] == i:\n",
        "              result.append(j)\n",
        "    return result\n",
        "\n",
        "\n",
        "def cos_sim(vector_a, vector_b):\n",
        "    \"\"\"\n",
        "    :param vector_a:  a\n",
        "    :param vector_b:  b\n",
        "    :return: sim\n",
        "    \"\"\"\n",
        "    vector_a = np.mat(vector_a)\n",
        "    vector_b = np.mat(vector_b)\n",
        "    num = float(vector_a * vector_b.T)\n",
        "    denom = np.linalg.norm(vector_a) * np.linalg.norm(vector_b)\n",
        "    cos = num / denom\n",
        "    sim = 0.5 + 0.5 * cos\n",
        "    return sim\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ai2AruYee07p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "top_five_cos_sim()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JkT7D25cpdnp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#3. Task3: Probabilistic Document Retrieval"
      ]
    },
    {
      "metadata": {
        "id": "BOAd9ZkMyoUf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "def traverse_file_each(list_dir):\n",
        "  for file in list_dir:\n",
        "    print(\"file name is :\" +str(file))\n",
        "    csv=pd.DataFrame()\n",
        "    test=open('/drive/My Drive/IRDM/wiki-pages/wiki-pages/'+file)  \n",
        "    for line in test:\n",
        "      fre_dict={}\n",
        "      lines_arr=[]\n",
        "      json_obj=json.loads(line)\n",
        "      lines=json_obj['lines'].split('\\n')\n",
        "      for i in range(len(lines)):\n",
        "        temp=lines[i][2:]\n",
        "        if len(temp)>1:\n",
        "          lines_arr.append(temp\n",
        "      tokens=get_tokens(json_obj['text'])\n",
        "      lemmatizer=WordNetLemmatizer()\n",
        "      line=lemmatized_tokens(tokens, lemmatizer)\n",
        "      for term in line:\n",
        "        if term in fre_dict:\n",
        "          fre_dict[term]=fre_dict[term]+1\n",
        "        else:\n",
        "          fre_dict[term]=1\n",
        "      csv=csv.append({'id':json_obj['id'],'freq':fre_dict,'lines':lines_arr},ignore_index=True)\n",
        "    csv.to_csv('/drive/My Drive/IRDM/task3/document_new/'+file+'.csv')\n",
        "    test.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bfL1fPQIyun9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "traverse_file_each(file_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f3Y-7RwyTTdb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Query-likelyhood"
      ]
    },
    {
      "metadata": {
        "id": "iWzjVpMwp56e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def query_likelyhood()\n",
        "  all_docu = os.listdir('/drive/My Drive/IRDM/task3/document/')\n",
        "  document_result={}\n",
        "  claim=open('/drive/My Drive/IRDM/train_temp.jsonl')\n",
        "  claim_dict={}\n",
        "\n",
        "  for line in claim:\n",
        "    json_obj=json.loads(line)\n",
        "    line=get_tokens(json_obj['claim'])\n",
        "    claim_dict[json_obj['id']]=line\n",
        "    document_result[json_obj['id']]={}\n",
        "\n",
        "  for document in all_docu:\n",
        "    print(document)\n",
        "    all_freq={}\n",
        "    temp_csv=pd.read_csv('/drive/My Drive/IRDM/task3/document/'+document)\n",
        "    temp_freq=temp_csv['freq'].values\n",
        "    temp_id=temp_csv['id'].values\n",
        "    for i in range(len(temp_freq)):\n",
        "      all_freq[temp_id[i]]=eval(temp_freq[i])\n",
        "    for one_claim in claim_dict.keys():\n",
        "        for document in all_freq.keys():\n",
        "          document_result[one_claim][document]=1\n",
        "          temp_total=np.int64(np.sum(list(all_freq[document].values())))\n",
        "          for word in claim_dict[one_claim]:\n",
        "            if word in all_freq[document].keys():\n",
        "              document_result[one_claim][document]= document_result[one_claim][document]*all_freq[document][word]/temp_total\n",
        "            else:\n",
        "              document_result[one_claim][document]=0\n",
        "              break\n",
        "  save_dict('/drive/My Drive/IRDM/task3/task3_query_dict.npy',document_result)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PLeptamP42yQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "query_likelyhood()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vk3dj9FaUsoe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Laplace smoothing"
      ]
    },
    {
      "metadata": {
        "id": "kukiw7RL92FS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def laplace_smoothing(file_list):\n",
        "  vocab_size=len(load_dict('/drive/My Drive/IRDM/task1/task1_dict_1.npy').keys())\n",
        "  for document in file_list:\n",
        "    document_result={}\n",
        "    claim=open('/drive/My Drive/IRDM/train_temp.jsonl')\n",
        "    claim_dict={}\n",
        "    for line in claim:\n",
        "      json_obj=json.loads(line)\n",
        "      line=get_tokens(json_obj['claim'])\n",
        "      claim_dict[json_obj['id']]=line\n",
        "      document_result[json_obj['id']]={}  \n",
        "    print(document)\n",
        "    document_score={}\n",
        "    all_freq={}\n",
        "    temp_csv=pd.read_csv('/drive/My Drive/IRDM/task3/document/'+document)\n",
        "    temp_freq=temp_csv['freq'].values\n",
        "    temp_id=temp_csv['id'].values\n",
        "    for i in range(len(temp_freq)):\n",
        "      all_freq[temp_id[i]]=eval(temp_freq[i])\n",
        "      document_score[temp_id[i]]=1\n",
        "    for single_claim in claim_dict.keys():\n",
        "        for doc in all_freq.keys():\n",
        "          document_result[single_claim][doc]=1\n",
        "          temp_total=np.int64(np.sum(list(all_freq[doc].values())))\n",
        "\n",
        "          if(temp_total+vocab_size)==0:\n",
        "            document_result[single_claim][doc]=0\n",
        "            continue\n",
        "          for word in claim_dict[single_claim]:\n",
        "\n",
        "            if word in all_freq[doc].keys():\n",
        "              document_result[single_claim][doc]= document_result[single_claim][doc]*(all_freq[doc][word]+1)/(temp_total+vocab_size)\n",
        "            else:\n",
        "              document_result[single_claim][doc]= document_result[single_claim][doc]*(0+1)/(temp_total+vocab_size)\n",
        "    save_dict('/drive/My Drive/IRDM/task3/laplace/task3_laplace_dict_'+document+'.npy',document_result)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "13jbILfh5Cvr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "file_list = os.listdir('/drive/My Drive/IRDM/task3/document/')\n",
        "laplace_smoothing(file_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GyjhaeFYUxF7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Jelinek-Mercer Smoothing"
      ]
    },
    {
      "metadata": {
        "id": "jkQEJ54t98ZP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def jel_smoothing(file_list):\n",
        "  vocab_size=len(load_dict('/drive/My Drive/IRDM/task1/task1_dict.npy').keys())\n",
        "  for document in file_list:\n",
        "    print(document)\n",
        "    document_score={}\n",
        "    all_freq={}\n",
        "    document_result={}\n",
        "    claim=open('/drive/My Drive/IRDM/train_temp.jsonl','r')\n",
        "    claim_dict={}\n",
        "    for line in claim:\n",
        "      json_obj=json.loads(line)\n",
        "      line=get_tokens(json_obj['claim'])\n",
        "      claim_dict[json_obj['id']]=line\n",
        "      document_result[json_obj['id']]={}\n",
        "    temp_csv=pd.read_csv('/drive/My Drive/IRDM/task3/document/'+document,chunksize=30000)\n",
        "    for chunk in temp_csv:\n",
        "      temp_freq=chunk['freq'].values\n",
        "      temp_id=chunk['id'].values\n",
        "      for i in range(len(temp_freq)):\n",
        "        all_freq[temp_id[i]]=eval(temp_freq[i])\n",
        "        document_score[temp_id[i]]=1\n",
        "    all_freq[0]=load_dict('/drive/My Drive/IRDM/task1/task1_dict.npy')\n",
        "  \n",
        "  \n",
        "    lamb=1/2\n",
        "    V=np.int64(np.sum(list(all_freq[0].values())))\n",
        "    \n",
        "    for one_claim in claim_dict.keys():\n",
        "        for doc in all_freq.keys():\n",
        "          if(doc==0):\n",
        "            continue\n",
        "          document_result[one_claim][doc]=1\n",
        "          temp_total=np.int64(np.sum(list(all_freq[doc].values())))\n",
        "\n",
        "          for word in claim_dict[one_claim]:\n",
        "\n",
        "            if word in all_freq[doc].keys():\n",
        "              if word in all_freq[0]:\n",
        "                document_result[one_claim][doc]= document_result[one_claim][doc]*((1-lamb)*(all_freq[doc][word]+1)/(temp_total+vocab_size)+lamb*all_freq[0][word]/V)\n",
        "              else:\n",
        "                document_result[one_claim][doc]= document_result[one_claim][doc]*((1-lamb)*(all_freq[doc][word]+1)/(temp_total+vocab_size))\n",
        "            else:\n",
        "              if word in all_freq[0]:\n",
        "                document_result[one_claim][doc]= document_result[one_claim][doc]*((1-lamb)*(0+1)/(temp_total+vocab_size)+lamb*all_freq[0][word]/V)\n",
        "              else:\n",
        "                document_result[one_claim][doc]= document_result[one_claim][doc]*((1-lamb)*(0+1)/(temp_total+vocab_size))\n",
        "    \n",
        "    save_dict('/drive/My Drive/IRDM/task3/jel/task3_jel_dict_'+str(document)+'.npy',document_result) \n",
        "\n",
        "file_list = os.listdir('/drive/My Drive/IRDM/task3/document/')\n",
        "jel_smoothing(file_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AK0wZkLNU7Am",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##dirichlet Smoothing"
      ]
    },
    {
      "metadata": {
        "id": "E8bKDXxJ-Dkt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def dirichlet_smoothing(file_list):\n",
        "  for document in documents_traverse:\n",
        "    print(document)\n",
        "    document_score={}\n",
        "    all_freq={}\n",
        "    document_result={}\n",
        "    claim=open('/drive/My Drive/IRDM/train_temp.jsonl')\n",
        "    claim_dict={}\n",
        "    for line in claim:\n",
        "      json_obj=json.loads(line)\n",
        "      line=get_tokens(json_obj['claim'])\n",
        "      claim_dict[json_obj['id']]=line\n",
        "      document_result[json_obj['id']]={}\n",
        "    temp_csv=pd.read_csv('/drive/My Drive/IRDM/task3/document/'+document)\n",
        "    \n",
        "    for part in temp_csv:\n",
        "      temp_freq=part['freq'].values\n",
        "      temp_id=part['id'].values\n",
        "      for i in range(len(temp_freq)):\n",
        "        all_freq[temp_id[i]]=eval(temp_freq[i])\n",
        "        document_score[temp_id[i]]=1\n",
        "    all_freq[0]=load_dict('/drive/My Drive/IRDM/task1/task1_dict.npy')\n",
        "    \n",
        "    mu=np.int64(np.sum(list(all_freq[0].values())))/(len(documents_traverse)*50000)\n",
        "    V=np.int64(np.sum(list(all_freq[0].values())))\n",
        "    for one_claim in claim_dict.keys():\n",
        "        count=0\n",
        "        for doc in all_freq.keys():\n",
        "          if(doc==0):\n",
        "            continue\n",
        "          document_result[one_claim][doc]=1\n",
        "          temp_total=np.int64(np.sum(list(all_freq[doc].values())))\n",
        "          N=temp_total\n",
        "          lamb=1-N/(N+mu)\n",
        "          for word in claim_dict[one_claim]:\n",
        "\n",
        "            if word in all_freq[doc].keys():\n",
        "              if word in all_freq[0]:\n",
        "                document_result[one_claim][doc]= document_result[one_claim][doc]*((1-lamb)*(all_freq[doc][word])/temp_total+lamb*all_freq[0][word]/V)\n",
        "              else:\n",
        "                document_result[one_claim][doc]= document_result[one_claim][doc]*((1-lamb)*(all_freq[doc][word])/temp_total)\n",
        "            else:\n",
        "              if word in all_freq[0]:\n",
        "                document_result[one_claim][doc]= document_result[one_claim][doc]*(lamb*all_freq[0][word]/V)\n",
        "              else:\n",
        "                document_result[one_claim][doc]= 0\n",
        "                break\n",
        "    \n",
        "    save_dict('/drive/My Drive/IRDM/task3/dirichlet/task3_Dirichlet_dict_'+document+'.npy',document_result) \n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "64yzKKtFUqgS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "file_list = os.listdir('/drive/My Drive/IRDM/task3/document/')    \n",
        "dirichlet_smoothing(file_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "15DJzCmFVLN3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Smoothing Analyze"
      ]
    },
    {
      "metadata": {
        "id": "gDLGdxZ_Bfy5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "query_dict=load_dict('/drive/My Drive/IRDM/task3/task3_query_dict.npy')\n",
        "for claim in query_dict.keys():\n",
        "  query_dict[claim]=sorted(query_dict[claim].items(),key=lambda item:item[1],reverse=True)\n",
        "  print(claim)\n",
        "  print(query_dict[claim][:5])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gHRY3syC-TjY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"function for task3 smoothing analysis\"\"\"\n",
        "\n",
        "def find_five_max(path):\n",
        "  docu_traverse = os.listdir(path)\n",
        "  claim=dict.fromkeys([75397, 150448, 214861, 156709, 129629, 33078, 6744, 226034, 40190, 76253])\n",
        "  for key in claim.keys():\n",
        "      claim[key]=[]\n",
        "  for docu in docu_traverse:\n",
        "    temp_dict=load_dict(path+str(docu))\n",
        "    for key in temp_dict.keys():\n",
        "        ranked = sorted(temp_dict[key].items(), key=lambda item: item[1], reverse=True)\n",
        "        claim[key]=claim[key]+ranked[:5]\n",
        "    \n",
        "  for key in claim.keys():\n",
        "      claim[key]=sorted(claim[key],key=lambda item:item[1],reverse=True)\n",
        "      print(key)\n",
        "      for item in claim[key][:5]\n",
        "        print(item+\";\")\n",
        "      print(\" \")\n",
        "  \n",
        "\n",
        "# Laplace smoothing\n",
        "find_five_max('/drive/My Drive/IRDM/task3/laplace/')\n",
        "\n",
        "#  Jelinek-MercerSmoothing \n",
        "find_five_max('/drive/My Drive/IRDM/task3/jel/')\n",
        "\n",
        "#  Dirichlet Smoothing \n",
        "find_five_max('./drive/My Drive/IRDM/task3/dirichlet/')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fOqlGjNx0v4J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Task4"
      ]
    },
    {
      "metadata": {
        "id": "9yk1oyDMacbl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Trainset generation"
      ]
    },
    {
      "metadata": {
        "id": "nxKDJAXmU9ai",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def task4_train_set_generate(documents):\n",
        "  for document in documents:\n",
        "    print(document)\n",
        "    train_set=pd.DataFrame()\n",
        "    temp_csv=pd.read_csv('/drive/My Drive/IRDM/task3/document/'+document)\n",
        "    train=open('/drive/My Drive/IRDM/train.jsonl','r')\n",
        "    length=len(temp_csv)\n",
        "    count=0\n",
        "    for line in train:\n",
        "      count+=1\n",
        "      if(count%1000==0):\n",
        "        print(\"current line: \"+str(count))\n",
        "      json_obj=json.loads(line)\n",
        "      if json_obj['verifiable']!='VERIFIABLE':\n",
        "        continue\n",
        "      else:\n",
        "        evidence=json_obj['evidence']\n",
        "        #print(evidence)\n",
        "        info={}\n",
        "        for item in evidence:\n",
        "          for sub_item in item:\n",
        "            if sub_item[2] in info:\n",
        "              info[sub_item[2]].append(sub_item[3])\n",
        "            else:\n",
        "              info[sub_item[2]]=[]\n",
        "              info[sub_item[2]].append(sub_item[3])\n",
        "              \n",
        "        for id_1 in info.keys():\n",
        "          sentence_line=temp_csv[temp_csv['id']==id_1]\n",
        "          if len(sentence_line)!=1:\n",
        "            continue\n",
        "          else:\n",
        "            sentence=''\n",
        "            for line_num in info[id_1]:\n",
        "              arr=eval(sentence_line['lines'].values[0])\n",
        "              if(line_num>=len(arr)):\n",
        "                continue\n",
        "              sentence=sentence+arr[line_num]\n",
        "            r_idx=random.randint(length)\n",
        "            negative_line=eval(temp_csv.iloc[r_idx]['lines'])\n",
        "            while len(negative_line)==0:\n",
        "              r_idx=random.randint(length)\n",
        "              negative_line=eval(temp_csv.iloc[r_idx]['lines'])\n",
        "            neg_sentence=negative_line[random.randint(len(negative_line))]\n",
        "            while neg_sentence==sentence:\n",
        "              r_idx=random.randint(length)\n",
        "              negative_line=eval(temp_csv.iloc[r_idx]['lines'])\n",
        "\n",
        "            neg_sentence=negative_line[random.randint(len(negative_line))]\n",
        "            train_set=train_set.append({'claim':json_obj['claim'],'sentence':sentence,'label':1},ignore_index=True)\n",
        "            train_set=train_set.append({'claim':json_obj['claim'],'sentence':neg_sentence,'label':0},ignore_index=True)\n",
        "    train_set.to_csv('/drive/My Drive/IRDM/task4/task4_trainset/'+document[:8]+'_train.csv')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PwODssmLYY9F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_train_set(bound1,bound2)\n",
        "    docu_traverse = os.listdir('/drive/My Drive/IRDM/task3/document/')\n",
        "    docu_traverse=sorted(docu_traverse)\n",
        "    bound=[bound1,bound2]\n",
        "    documents=documents_traverse[bound[0]:bound[1]]\n",
        "    task4_train_set_generate(documents)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J5m4_qjkY29_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def merge_file(path_input,output_file):\n",
        "  output = r'/drive/My Drive/IRDM/'+output_file\n",
        "  file_list = os.listdir('/drive/My Drive/IRDM/'+path_input) \n",
        "  df = pd.read_csv('/drive/My Drive/IRDM/'+path_input+\"/\"+file_list[0])\n",
        "  df=df.drop_duplicates(['claim','label','sentence'])\n",
        "  df.to_csv(output, encoding=\"utf_8\", index=False) \n",
        "  for i in range(1, len(file_list)):\n",
        "      df = pd.read_csv('/drive/My Drive/IRDM/'+path'/'+file_list[i])\n",
        "      df=df.drop_duplicates(['claim','label','sentence'])\n",
        "      df.to_csv(output,encoding='utf-8' index=False, header=False, mode='a+')\n",
        "\n",
        "merge_file('/task4/','tas4_train.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gzdgylcLaqdp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Word embedding"
      ]
    },
    {
      "metadata": {
        "id": "VE9dRHxc07j1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def word_averaging(wv, words):\n",
        "    mean = []\n",
        "    contain_wv_word = False\n",
        "    for word in words:\n",
        "        if word in wv.vocab:\n",
        "            contain_wv_word = True\n",
        "            mean.append(wv.vectors_norm[wv.vocab[word].index])\n",
        "    if not contain_wv_word: #unknown word for pre-trained word2vec data\n",
        "        mean.append(wv.vectors_norm[wv.vocab[\"apple\"].index])\n",
        "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
        "    return mean\n",
        "\n",
        "def word_averaging_list(wv, text_list):\n",
        "  return np.vstack([word_averaging(wv, review) for review in text_list ])\n",
        "\n",
        "def tokensation(text):\n",
        "    tokens = []\n",
        "    for sent in nltk.sent_tokenize(text, language='english'):\n",
        "        for word in nltk.word_tokenize(sent, language='english'):\n",
        "            tokens.append(word)\n",
        "    return tokens\n",
        "  \n",
        "def x_to_word2vec(df,word2vec_model):\n",
        "    claim_list = df.apply(lambda r: tokensation(r['claim']), axis=1).values\n",
        "    sentence_list = df.apply(lambda r: tokensation(r['sentence']), axis=1).values\n",
        "    claim = word_averaging_list(word2vec_model,claim_list)\n",
        "    sentence = word_averaging_list(word2vec_model,sentence_list)\n",
        "    return np.hstack([claim,sentence])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y0dAyMGcapT8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "word2vec_model_path = '/drive/My Drive/IRDM/word2vec/glove.6B.50d.txt' \n",
        "word2vec_model = KeyedVectors.load_word2vec_format(word2vec_model_path, binary=False,unicode_errors='ignore',encoding='utf-8')\n",
        "word2vec_model.init_sims(replace=True)\n",
        "df = pd.read_csv('/drive/My Drive/IRDM//task4/tas4_train.csv')\n",
        "df = df.dropna(how='any',axis=0) \n",
        "X=x_to_word2vec(df,word2vec_model)\n",
        "Y=df['label'].values\n",
        "X_train,X_valid,y_train,y_valid = train_test_split(X,Y)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UbC9BTOTbK70",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Logistic Regression"
      ]
    },
    {
      "metadata": {
        "id": "Mys_3tub1DYL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Logistic_Regression:\n",
        "    def _init__(self):\n",
        "        self.coef = None\n",
        "        self.intercept = None\n",
        "        self.theta = None\n",
        "        \n",
        "    def save(self,path):\n",
        "        np.save(path+'coef.npy',self.coef)\n",
        "        np.save(path+'intercept.npy',self.intercept)\n",
        "        np.save(path+'theta.npy',self.theta)\n",
        "        \n",
        "    def load(self,path):\n",
        "        self.coef = np.load(path+'coef.npy')\n",
        "        self.intercept = np.load(path+'intercept.npy')\n",
        "        self.theta = np.load(path+'theta.npy')\n",
        "        \n",
        "    def sigmod(self, t):\n",
        "        return 1. / (1. + np.exp(-t))\n",
        "\n",
        "    def loss(self,theta, X_b, y):\n",
        "        y_hat = self.sigmod(X_b.dot(theta))\n",
        "        return -1 * (np.sum(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))) / len(y)\n",
        "\n",
        "    def gradient(self,theta, X_b, y):\n",
        "        y_hat = self.sigmod(X_b.dot(theta))\n",
        "        return X_b.T.dot(y_hat - y) / len(y)\n",
        "\n",
        "    def gradient_descent(self,X_b, y, initial_theta, eta, n_iters, epsilon=1e-6):\n",
        "        theta = initial_theta\n",
        "        cur_iter = 0\n",
        "        while cur_iter < n_iters:\n",
        "            gradient = self.gradient(theta, X_b, y)\n",
        "            pre_theta = theta\n",
        "            theta = theta-eta* gradient\n",
        "            if(cur_iter%100==0):\n",
        "              file=open(\"/drive/My Drive/IRDM/task4/model_loss_\"+str(eta)+\".txt\",'a')\n",
        "              file.write(\"iter:\"+str(cur_iter)+\";\"+str(abs(self.loss(theta, X_b, y)))+\"\\n\")\n",
        "              print(\"iter:\",cur_iter, \" : \", abs(self.loss(theta, X_b, y)))\n",
        "              file.close()\n",
        "            if abs(self.loss(theta, X_b, y) - self.loss(pre_theta, X_b, y)) < epsilon:\n",
        "                break\n",
        "            cur_iter += 1\n",
        "        return theta\n",
        "     \n",
        "    def fit(self, X_train, y_train, eta=1, n_iters=1e6):\n",
        "        X_b = np.hstack([np.ones((len(X_train), 1)), X_train])\n",
        "        initial_theta = np.zeros(X_b.shape[1])\n",
        "        self.theta = self.gradient_descent(X_b, y_train, initial_theta, eta, n_iters)\n",
        "        self.intercept = self.theta[0]\n",
        "        self.coef = self.theta[1:]\n",
        "\n",
        "    def predict_proability(self, X):\n",
        "        X_b = np.hstack([np.ones((len(X), 1)), X])\n",
        "        return self.sigmod(X_b.dot(self.theta))\n",
        "\n",
        "    def get_classification(self, X):\n",
        "        proability = self.predict_proability(X)\n",
        "        res=[]\n",
        "        for prob in proability:\n",
        "            if prob>=0.5:\n",
        "                res.append(1)\n",
        "            else:\n",
        "                res.append(0)\n",
        "        return np.array(res, dtype='int')\n",
        "\n",
        "    def Accuracy(self, X, y):\n",
        "        y_predict = self.predict_proability(X)\n",
        "        accu=0\n",
        "        for i in range(len(y_predict)):\n",
        "          if(y_predict[i]>=0.5 and y[i]==1):\n",
        "            accu+=1\n",
        "          if(y_predict[i]<0.5 and y[i]==0):\n",
        "            accu+=1\n",
        "        print('Accurately predicted:' +str(accu))\n",
        "        print('total: '+str(len(y_predict)))\n",
        "        return accu/ len(y_predict)\n",
        "        \n",
        "    def True_Negative(self, X, y):\n",
        "        y_predict = self.get_classification(X)\n",
        "        sum_z_z=0\n",
        "        for i in range(len(y)):\n",
        "          if(y[i]==0 and y_predict[i]==0):\n",
        "            sum_z_z+=1\n",
        "        return sum_z_z\n",
        "\n",
        "    def False_Negative(self, X, y):\n",
        "        y_predict = self.get_classification(X)\n",
        "        sum_o_z=0\n",
        "        for i in range(len(y)):\n",
        "          if(y[i]==1 and y_predict[i]==0):\n",
        "            sum_o_z+=1\n",
        "        return sum_o_z\n",
        "\n",
        "    def True_Positive(self, X, y):\n",
        "        y_predict = self.get_classification(X)\n",
        "        sum_o_o=0\n",
        "        for i in range(len(y)):\n",
        "          if(y[i]==1 and y_predict[i]==1):\n",
        "            sum_o_o+=1\n",
        "        return sum_o_o\n",
        "\n",
        "    def False_Positive(self, X, y):\n",
        "        y_predict = self.get_classification(X)\n",
        "        sum_z_o=0\n",
        "        for i in range(len(y)):\n",
        "          if(y[i]==0 and y_predict[i]==1):\n",
        "            sum_z_o+=1\n",
        "        return sum_z_o\n",
        "\n",
        "    def precision(self,X,y):\n",
        "        tp=self.True_Positive(X,y)\n",
        "        fp=self.False_Positive(X,y)\n",
        "        return tp/(tp+fp)\n",
        "\n",
        "    def recall(self,X,y):\n",
        "        tp=self.True_Positive(X,y)\n",
        "        fn=self.False_Negative(X,y)\n",
        "        return tp/(tp+fn)\n",
        "\n",
        "    def f1_score(self,X,y):\n",
        "        return 2*self.precision(X,y)*self.recall(X,y)/(self.precision(X,y)+self.recall(X,y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s_-JWZR8bfDj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Train model"
      ]
    },
    {
      "metadata": {
        "id": "nNImI7HyKozX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "logistic_regression = Logistic_Regression()\n",
        "logistic_regression.fit(X_train,y_train,n_iters=100000,eta=1)\n",
        "logistic_regression.save('/drive/My Drive/IRDM/task4/LR_model/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V8Q3uA441NXZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "logistic_regression= LogisticRegression()\n",
        "logistic_regression.load('/drive/My Drive/IRDM/task4/LR_model/')\n",
        "\n",
        "print (\"Validation Accuracy:\",logistic_regression.accuracy(X_valid,y_valid))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xih9PozpcY9_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Test model"
      ]
    },
    {
      "metadata": {
        "id": "v2ruAqgK1N_o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from numpy import random\n",
        "\n",
        "def task4_pre(documents):\n",
        "  for document in documents:\n",
        "    print(document)\n",
        "    train_set=pd.DataFrame()\n",
        "    temp_csv=pd.read_csv('/drive/My Drive/IRDM/task3/document/'+document)\n",
        "    train=open('/drive/My Drive/IRDM/shared_task_dev.jsonl','r')\n",
        "    length=len(temp_csv)\n",
        "    \n",
        "    \n",
        "    for line in train:\n",
        "      json_obj=json.loads(line)\n",
        "      if json_obj['verifiable']!='VERIFIABLE':\n",
        "        continue\n",
        "      elif json_obj['id'] in [137334, 111897, 89891, 181634, 219028, 108281, 204361, 54168, 105095, 18708]:\n",
        "        evidence=json_obj['evidence']\n",
        "        dict_temp={}\n",
        "        for item in evidence:\n",
        "          for sub_item in item:\n",
        "            if sub_item[2] in dict_temp:\n",
        "              info[sub_item[2]].append(sub_item[3])\n",
        "            else:\n",
        "              info[sub_item[2]]=[]\n",
        "              info[sub_item[2]].append(sub_item[3])\n",
        "              \n",
        "              \n",
        "              \n",
        "        for id_1 in dict_temp.keys():\n",
        "          sentence_line=temp_csv[temp_csv['id']==id_1]\n",
        "          if len(sentence_line)!=1:\n",
        "            continue\n",
        "          else:\n",
        "            sentence=''\n",
        "            for line_num in dict_temp[id_1]:\n",
        "              arr=eval(sentence_line['lines'].values[0])\n",
        "              if(line_num>=len(arr)):\n",
        "                continue\n",
        "              sentence=sentence+arr[line_num]\n",
        "            r_idx=random.randint(length)\n",
        "            negative_line=eval(temp_csv.iloc[r_idx]['lines'])\n",
        "            while len(negative_line)==0:\n",
        "              r_idx=random.randint(length)\n",
        "              negative_line=eval(temp_csv.iloc[r_idx]['lines'])\n",
        "            neg_sentence=negative_line[random.randint(len(negative_line))]\n",
        "            while neg_sentence==sentence:\n",
        "              r_idx=random.randint(length)\n",
        "              negative_line=eval(temp_csv.iloc[r_idx]['lines'])\n",
        "\n",
        "            neg_sentence=negative_line[random.randint(len(negative_line))]\n",
        "            train_set=train_set.append({'claim':json_obj['claim'],'sentence':sentence,'label':1},ignore_index=True)\n",
        "            train_set=train_set.append({'claim':json_obj['claim'],'sentence':neg_sentence,'label':0},ignore_index=True)\n",
        "    train_set.to_csv('/drive/My Drive/IRDM/task4/task4_testset/'+document[:8]+'_test.csv')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KHt99Fmbd-aS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "document_all = os.listdir('/drive/My Drive/IRDM/task3/document/')\n",
        "document_all=sorted(document_all)\n",
        "bound=[0,len(document_all)]\n",
        "documents=document_all[bound[0]:bound[1]]\n",
        "pre_processing_test_t4(documents)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hbB8_uFFFXrm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def merge_test_test4()\n",
        "    output = r'/drive/My Drive/IRDM/task4/task4_test.csv' \n",
        "    file_list = os.listdir('/drive/My Drive/IRDM/task4/task4_testset/')\n",
        "    df = pd.read_csv('/drive/My Drive/IRDM/task4/task4_testset/'+file_list[0])\n",
        "    df['claim']=None\n",
        "    df['label']=None\n",
        "    df['sentence']=None\n",
        "    df = df.dropna(how='any',axis=0) \n",
        "    df.to_csv(output, encoding=\"utf_8\", index=False) \n",
        "    for i in range(1, len(file_list)):\n",
        "        df = pd.read_csv('/drive/My Drive/IRDM/task4/task4_testset/'+file_list[i])\n",
        "        df = df.dropna(how='any',axis=0) \n",
        "        df.to_csv(output, encoding=\"utf_8\", index=False, header=False, mode='a+')\n",
        "        \n",
        "merge_test_test4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WxpmYw23f0Dw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "PeS844-E1Rql",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "###Validation\n",
        "test_set=pd.read_csv('/drive/My Drive/IRDM/task4/task4_test.csv')\n",
        "X=x_to_word2vec(test_set,word2vec_model)\n",
        "Y=test_set['label'].values\n",
        "logistic_regression= Logistic_Regression()\n",
        "logistic_regression.load('/drive/My Drive/IRDM/task4/LR_model/')\n",
        "y_prob=logistic_regression.predict_proability(X)\n",
        "y_predict=logistic_regression.get_classification(X)\n",
        "claims=test_set['claim'].values\n",
        "sentence=test_set['sentence'].values\n",
        "for i in range(len(y_prob)):\n",
        "  if(i%100==0):\n",
        "    print('label: ',Y[i],'probability: ',y_prob[i],'classification: ',y_predict[i],'claim: ',claims[i],'sentence: ',sentence[i])\n",
        "    print(\"!!!!\")\n",
        "    \n",
        "print (\"Validation Accuracy:\",logistic_regression.Accuracy(X,Y))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ywTeGmNb7lyn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#5. Task5"
      ]
    },
    {
      "metadata": {
        "id": "5j9fTQAriVJb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Testset Generation"
      ]
    },
    {
      "metadata": {
        "id": "KIIBlX22EMf7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "from numpy import random\n",
        "\n",
        "def task5_testset_generation(documents):\n",
        "  for document in documents:\n",
        "    print(document)\n",
        "    train_set=pd.DataFrame()\n",
        "    temp_csv=pd.read_csv('/drive/My Drive/IRDM/task3/document/'+document)\n",
        "    train=open('/drive/My Drive/IRDM/shared_task_dev.jsonl','r')\n",
        "    length=len(temp_csv)\n",
        "    for line in train:\n",
        "      json_obj=json.loads(line)\n",
        "      if json_obj['verifiable']!='VERIFIABLE':\n",
        "        continue\n",
        "      else:\n",
        "        evidence=json_obj['evidence']\n",
        "        info={}\n",
        "        for item in evidence:\n",
        "          for sub_item in item:\n",
        "            if sub_item[2] in info:\n",
        "              info[sub_item[2]].append(sub_item[3])\n",
        "            else:\n",
        "              info[sub_item[2]]=[]\n",
        "              info[sub_item[2]].append(sub_item[3])\n",
        "        for id in info.keys():\n",
        "          sentence_line=temp_csv[temp_csv['id']==id]\n",
        "          if len(sentence_line)!=1:\n",
        "            continue\n",
        "          else:\n",
        "            sentence=''\n",
        "            for line_num in info[id]:\n",
        "              arr=eval(sentence_line['lines'].values[0])\n",
        "              if(line_num>=len(arr)):\n",
        "                continue\n",
        "              sentence=sentence+arr[line_num]\n",
        "            r_idx=random.randint(length)\n",
        "            negative_line=eval(temp_csv.iloc[r_idx]['lines'])\n",
        "            while len(negative_line)==0:\n",
        "              r_idx=random.randint(length)\n",
        "              negative_line=eval(temp_csv.iloc[r_idx]['lines'])\n",
        "            neg_sentence=negative_line[random.randint(len(negative_line))]\n",
        "            while neg_sentence==sentence:\n",
        "              r_idx=random.randint(length)\n",
        "              negative_line=eval(temp_csv.iloc[r_idx]['lines'])\n",
        "\n",
        "            neg_sentence=negative_line[random.randint(len(negative_line))]\n",
        "            train_set=train_set.append({'claim':json_obj['claim'],'sentence':sentence,'label':1},ignore_index=True)\n",
        "            train_set=train_set.append({'claim':json_obj['claim'],'sentence':neg_sentence,'label':0},ignore_index=True)\n",
        "    train_set.to_csv('/drive/My Drive/IRDM/task5/testset/'+document[:8]+'_test.csv')\n",
        "\n",
        "    \n",
        "def generate_train_set_t5(bound1,bound2)\n",
        "    docu_all = os.listdir('/drive/My Drive/IRDM/task3/document/')\n",
        "    docu_all=sorted(docu_all)\n",
        "    documents=docu_all[bound1:bound2]\n",
        "    task5_testset_generation(documents)\n",
        "\n",
        "generate_train_set_t5(1,109)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7n14mxKVlb1A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def merge_file2(inputpath,outputfile)\n",
        "    output = r'/drive/My Drive/IRDM/'+outputfile\n",
        "    file_list = os.listdir('/drive/My Drive/IRDM/'+inputpath)\n",
        "    df = pd.read_csv('/drive/My Drive/IRDM/task5/testset/'+file_list[0])\n",
        "    df['claim']=None\n",
        "    df['label']=None\n",
        "    df['sentence']=None\n",
        "    df = df.dropna(how='any',axis=0) \n",
        "    df.to_csv(output, encoding=\"utf_8\", index=False)\n",
        "    for i in range(1, len(file_list)):\n",
        "        print(i)\n",
        "        df = pd.read_csv('/drive/My Drive/IRDM/'+inputpath'/'+file_list[i])\n",
        "        df = df.dropna(how='any',axis=0) \n",
        "        df.to_csv(output, encoding=\"utf_8\", index=False, header=False, mode='a+')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8LtVRZMX8mSL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "merge_file2('task5/testset','task5/test_task5.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "QELa2rb4lqOi",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1xz9GsdJj3tL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Test Model"
      ]
    },
    {
      "metadata": {
        "id": "9vLHRvAIi24U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def word_averaging(wv, words):\n",
        "    mean = []\n",
        "    contain_wv_word = False\n",
        "    for word in words:\n",
        "        if word in wv.vocab:\n",
        "            contain_wv_word = True\n",
        "            mean.append(wv.vectors_norm[wv.vocab[word].index])\n",
        "    if not contain_wv_word: #unknown word for pre-trained word2vec data\n",
        "        mean.append(wv.vectors_norm[wv.vocab[\"apple\"].index])\n",
        "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
        "    return mean\n",
        "\n",
        "def word_averaging_list(wv, text_list):\n",
        "  return np.vstack([word_averaging(wv, review) for review in text_list ])\n",
        "\n",
        "def tokensation(text):\n",
        "    tokens = []\n",
        "    for sent in nltk.sent_tokenize(text, language='english'):\n",
        "        for word in nltk.word_tokenize(sent, language='english'):\n",
        "            tokens.append(word)\n",
        "    return tokens\n",
        "  \n",
        "def x_to_word2vec(df,word2vec_model):\n",
        "    claim_list = df.apply(lambda r: tokensation(r['claim']), axis=1).values\n",
        "    sentence_list = df.apply(lambda r: tokensation(r['sentence']), axis=1).values\n",
        "    claim = word_averaging_list(word2vec_model,claim_list)\n",
        "    sentence = word_averaging_list(word2vec_model,sentence_list)\n",
        "    return np.hstack([claim,sentence])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iVb_eYZYk2Qz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Computer Precision Recall F1 in Task4's class Logistic Regression"
      ]
    },
    {
      "metadata": {
        "id": "CDLtQClG8rCk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test=pd.read_csv('/drive/My Drive/IRDM/task5/test_task5.csv')\n",
        "\n",
        "X=x_to_word2vec(test,word2vec_model)\n",
        "Y=test['label'].values\n",
        "logistic_regression= LogisticRegression()\n",
        "logistic_regression.load('/drive/My Drive/IRDM/task4/LR_model/')\n",
        "y_prob=logistic_regression.predict_proability(X)\n",
        "y_predict=logistic_regression.get_classification(X)\n",
        "print (\"Test Accuracy:\",logistic_regression.accuracy(X,Y)\n",
        "print(\"Precision :\",logistic_regression.precision(X,Y))\n",
        "print(\"Recall:\",logistic_regression.recall(X,Y))\n",
        "print(\"f1:\",logistic_regression.f1_score(X,Y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ycy0W3yKbb4U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Task6"
      ]
    },
    {
      "metadata": {
        "id": "F1JC4v97mfO2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Data preprocessing"
      ]
    },
    {
      "metadata": {
        "id": "xC8N3XUvbZPo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import time\n",
        "from numpy import random\n",
        "import pandas as pd\n",
        "\n",
        "def test6_trainset_generation(documents):\n",
        "  \n",
        "  \n",
        "  for document in documents:\n",
        "    print(document)\n",
        "    train_set=pd.DataFrame()\n",
        "    temp_csv=pd.read_csv('/drive/My Drive/IRDM/task3/document/'+document)\n",
        "    train=open('/drive/My Drive/IRDM/train.jsonl','r')\n",
        "    length=len(temp_csv)\n",
        "    for line in train:\n",
        "      json_obj=json.loads(line)\n",
        "      if json_obj['verifiable']!='VERIFIABLE':\n",
        "        continue\n",
        "      else:\n",
        "        evidence=json_obj['evidence']\n",
        "        info={}\n",
        "        for item in evidence:\n",
        "          for sub_item in item:\n",
        "            if sub_item[2] in info:\n",
        "              info[sub_item[2]].append(sub_item[3])\n",
        "            else:\n",
        "              info[sub_item[2]]=[]\n",
        "              info[sub_item[2]].append(sub_item[3])\n",
        "        for id in info.keys():\n",
        "          sentence_line=temp_csv[temp_csv['id']==id]\n",
        "          if len(sentence_line)!=1:\n",
        "            continue\n",
        "          else:\n",
        "            sentence=''\n",
        "            for line_num in info[id]:\n",
        "              arr=eval(sentence_line['lines'].values[0])\n",
        "              if(line_num>=len(arr)):\n",
        "                continue\n",
        "              sentence=sentence+arr[line_num]\n",
        "            \n",
        "            \n",
        "            label=0\n",
        "            if(json_obj['label']=='SUPPORTS'):\n",
        "              label=1\n",
        "            train_set=train_set.append({'claim':json_obj['claim'],'sentence':sentence,'label':label},ignore_index=True)\n",
        "\n",
        "    train_set.to_csv('/drive/My Drive/IRDM/task6/train_set/'+document[:8]+'_train.csv')\n",
        "\n",
        "    \n",
        "def generate_train_set_t6(bound1,bound2)\n",
        "    docu_all = os.listdir('/drive/My Drive/IRDM/task3/document/')\n",
        "    docu_all=sorted(docu_all)\n",
        "    documents=docu_all[bound1:bound2]\n",
        "    task5_testset_generation(documents)\n",
        "\n",
        "test6_trainset_generation(1,109)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vUJy50J2A0Lu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "merge_file2('task6a/train_task6.csv','task6/train_set/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y6sIjOgdFKVM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from numpy import random\n",
        "\n",
        "def pre_processing_t6_test(documents):\n",
        "  for document in documents:\n",
        "    print(document)\n",
        "    train_set=pd.DataFrame()\n",
        "    temp_csv=pd.read_csv('/drive/My Drive/IRDM/task3/document/'+document)\n",
        "    train=open('/drive/My Drive/IRDM/shared_task_dev.jsonl','r')\n",
        "    length=len(temp_csv)\n",
        "    for line in train:\n",
        "      json_obj=json.loads(line)\n",
        "      if json_obj['verifiable']!='VERIFIABLE':\n",
        "        continue\n",
        "      else:\n",
        "        evidence=json_obj['evidence']\n",
        "        info={}\n",
        "        for item in evidence:\n",
        "          for sub_item in item:\n",
        "            if sub_item[2] in info:\n",
        "              info[sub_item[2]].append(sub_item[3])\n",
        "            else:\n",
        "              info[sub_item[2]]=[]\n",
        "              info[sub_item[2]].append(sub_item[3])\n",
        "        for id in info.keys():\n",
        "          sentence_line=temp_csv[temp_csv['id']==id]\n",
        "          if len(sentence_line)!=1:\n",
        "            continue\n",
        "          else:\n",
        "            sentence=''\n",
        "            for line_num in info[id]:\n",
        "              arr=eval(sentence_line['lines'].values[0])\n",
        "              if(line_num>=len(arr)):\n",
        "                continue\n",
        "              sentence=sentence+arr[line_num]\n",
        "            sup=0\n",
        "            if(json_obj['label']=='SUPPORTS'):\n",
        "              sup=1\n",
        "            train_set=train_set.append({'claim':json_obj['claim'],'sentence':sentence,'label':sup},ignore_index=True)\n",
        "\n",
        "    train_set.to_csv('/drive/My Drive/IRDM/task6/test_set/'+document[:8]+'_test.csv')\n",
        "\n",
        "documents_traverse = os.listdir('/drive/My Drive/IRDM/task3/document/')\n",
        "documents_traverse=sorted(documents_traverse)\n",
        "bound=[0,len(documents_traverse)]\n",
        "documents=documents_traverse[bound[0]:bound[1]]\n",
        "pre_processing_t6_test(documents)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mcW0n5PiFLM4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "merge_file2('task6/test_task6.csv','task6/test_set/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NxmWt11TqbxT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Model"
      ]
    },
    {
      "metadata": {
        "id": "Ssy4oZDNCQT6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras import Model\n",
        "from keras.layers import *\n",
        "from keras.preprocessing import text,sequence\n",
        "\n",
        "class NN_Model():\n",
        "    def model(self, embeddings_matrix, maxlen_cl,maxlen_sen, word_index):\n",
        "        claim = Input(shape=(maxlen_cl,),name='claim')\n",
        "        sentence=Input(shape=(maxlen_sen,),name='sentence')\n",
        "        encode_c = CuDNNGRU(64, return_sequences=True)\n",
        "        encode_s= Bidirectional(CuDNNGRU(64, return_sequences=True))\n",
        "        encode2_s = Bidirectional(CuDNNGRU(64, return_sequences=True))\n",
        "        x_4_s = Embedding(len(word_index) + 1,\n",
        "                        embeddings_matrix.shape[1],\n",
        "                        weights=[embeddings_matrix],\n",
        "                        input_length=maxlen_sen,\n",
        "                        trainable=True)(sentence)\n",
        "        x_4_c = Embedding(len(word_index) + 1,\n",
        "                embeddings_matrix.shape[1],\n",
        "                weights=[embeddings_matrix],\n",
        "                input_length=maxlen_cl,\n",
        "                trainable=True)(claim)\n",
        "        x_3_s = SpatialDropout1D(0.2)(x_4_s)\n",
        "        x_3_s = encode_s(x_3_s)\n",
        "        x_3_s = Dropout(0.2)(x_3_s)\n",
        "        x_3_s = encode2_s(x_3_s)\n",
        "        x_3_s = Dropout(0.2)(x_3_s)\n",
        "        avg_pool_3_s = GlobalAveragePooling1D()(x_3_s)\n",
        "        max_pool_3_s = GlobalMaxPooling1D()(x_3_s)\n",
        "        \n",
        "        x_3_c = SpatialDropout1D(0.2)(x_4_c)\n",
        "        x_3_c = encode_c(x_3_c)\n",
        "        x_3_c = Dropout(0.2)(x_3_c)\n",
        "        avg_pool_3_c = GlobalAveragePooling1D()(x_3_c)\n",
        "        max_pool_3_c = GlobalMaxPooling1D()(x_3_c)\n",
        "        x = keras.layers.concatenate([avg_pool_3_s, max_pool_3_s,avg_pool_3_c, max_pool_3_c], name=\"fc\")\n",
        "        main_output = Dense(1, activation=\"sigmoid\",name='main_output')(x)\n",
        "        \n",
        "        adam = keras.optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-08,amsgrad=True)\n",
        "        rmsprop = keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-06)\n",
        "        model = Model(inputs=[claim,sentence], outputs=main_output)\n",
        "        model.compile(\n",
        "            loss='binary_crossentropy',\n",
        "            metrics = ['accuracy'],\n",
        "            optimizer=adam)\n",
        "        return model\n",
        "      \n",
        "from keras.callbacks import ModelCheckpoint, Callback\n",
        "from sklearn.metrics import f1_score, recall_score, precision_score\n",
        "\n",
        "def getClassification(prob):\n",
        "   \n",
        "    if prob<0.5:\n",
        "        return 0\n",
        "    else :\n",
        "        return 1\n",
        "\n",
        "class Metrics(Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.val_f1s = []\n",
        "        self.val_recalls = []\n",
        "        self.val_precisions = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        print('epoch end')\n",
        "        print(len(self.validation_data))\n",
        "        val_predict = list(map(getClassification, self.model.predict([self.validation_data[0],self.validation_data[1]])))\n",
        "        val_targ = list(self.validation_data[2])\n",
        "        _val_f1 = f1_score(val_targ, val_predict, average=\"macro\")\n",
        "        _val_recall = recall_score(val_targ, val_predict, average=\"macro\")\n",
        "        _val_precision = precision_score(val_targ, val_predict, average=\"macro\")\n",
        "        self.val_f1s.append(_val_f1)\n",
        "        self.val_recalls.append(_val_recall)\n",
        "        self.val_precisions.append(_val_precision)\n",
        "        print(_val_f1, _val_precision, _val_recall)\n",
        "        print(\"max f1\")\n",
        "        print(max(self.val_f1s))\n",
        "        return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D5ngg2GGqt6t",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Train and validation"
      ]
    },
    {
      "metadata": {
        "id": "QldppTuEDDsJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import plot_model\n",
        "import pandas as pd\n",
        "\n",
        "data=pd.read_csv('/drive/My Drive/IRDM/task6/train_task6.csv')\n",
        "model_dir='/drive/My Drive/IRDM/task6_model/'\n",
        "data['sentence'] = data.apply(lambda r: r['sentence'].replace('\\t',''), axis=1)\n",
        "x_train_c=data['claim'].values\n",
        "x_train_s=data['sentence'].values\n",
        "y_train=data['label'].values\n",
        "\n",
        "word2vec_model_path = '/drive/My Drive/IRDM/word2vec/glove.6B.50d.txt' \n",
        "word2vec_model = KeyedVectors.load_word2vec_format(word2vec_model_path, binary=False,unicode_errors='ignore',encoding='utf8')\n",
        "word2vec_model.init_sims(replace=True)\n",
        "\n",
        "tokenizer = text.Tokenizer(num_words=None)\n",
        "data_input=np.concatenate((data['claim'].values,data['sentence'].values),axis=0)\n",
        "tokenizer.fit_on_texts(data_input)\n",
        "word_index = tokenizer.word_index\n",
        "embeddings_matrix = np.zeros((len(word_index) + 1, word2vec_model.vector_size))\n",
        "vocab_list = [(k, word2vec_model[k]) for k, v in word2vec_model.vocab.items()]\n",
        "for word, i in word_index.items():\n",
        "    if word in word2vec_model:\n",
        "        embedding_vector = word2vec_model[word]\n",
        "        #print(embedding_vector)\n",
        "    else:\n",
        "        embedding_vector = None\n",
        "    if embedding_vector is not None:\n",
        "        embeddings_matrix[i] = embedding_vector\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fBX_xX8ZsXrI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "maxlen_s=500\n",
        "maxlen_c=50\n",
        "\n",
        "model = NN_Model().model(embeddings_matrix, maxlen_c,maxlen_s, word_index)\n",
        "\n",
        "file_path = model_dir + \"model_{epoch:02d}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(file_path, verbose=1, save_weights_only=True)\n",
        "metrics = Metrics()\n",
        "callbacks_list = [checkpoint, metrics]\n",
        "x_train_c = tokenizer.texts_to_sequences(x_train_c)\n",
        "x_train_c = sequence.pad_sequences(x_train_c, maxlen=maxlen_c)\n",
        "x_train_s = tokenizer.texts_to_sequences(x_train_s)\n",
        "x_train_s = sequence.pad_sequences(x_train_s, maxlen=maxlen_s)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DiogaLWAq1uL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "history = model.fit(x={'claim':x_train_c,'sentence':x_train_s},y={'main_output':y_train},validation_split=0.2, epochs=15,callbacks=callbacks_list, verbose=1,batch_size=512)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hnJVWkvQq5Xo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data=pd.read_csv('/drive/My Drive/IRDM/task6/test_task6.csv')\n",
        "\n",
        "data['sentence'] = data.apply(lambda r: sentence_clean(r['sentence']), axis=1)\n",
        "x_test_c=data['claim'].values\n",
        "x_test_s=data['sentence'].values\n",
        "y_test=data['label'].values\n",
        "\n",
        "\n",
        "maxlen_s=500\n",
        "maxlen_c=50\n",
        "\n",
        "callbacks_list = [checkpoint, metrics]\n",
        "x_test_c = tokenizer.texts_to_sequences(x_test_c)\n",
        "x_test_c = sequence.pad_sequences(x_test_c, maxlen=maxlen_c)\n",
        "x_test_s = tokenizer.texts_to_sequences(x_test_s)\n",
        "x_test_s = sequence.pad_sequences(x_test_s, maxlen=maxlen_s)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2NdzxsDerOOG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "best=NN_Model().model(embeddings_matrix, maxlen_c,maxlen_s, word_index)\n",
        "best.load_weights('./drive/My Drive/my_irdm/task5_model/model_04.hdf5')\n",
        "print(model_best.metrics_names)\n",
        "\n",
        "final = model_best.evaluate(x={'claim':x_test_c,'sentence':x_test_s},\n",
        "            y={'main_output': y_test},\n",
        "            batch_size=10, verbose=1)\n",
        "print(final)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZKi6h9nrmWSi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plot_model(model, to_file='/drive/My Drive/IRDM/model.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q17U-RMWmWy3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Task 8"
      ]
    },
    {
      "metadata": {
        "id": "oQIfZRqXJfbu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Attention"
      ]
    },
    {
      "metadata": {
        "id": "2IxoHubYmPSr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# coding=utf8\n",
        "from keras import backend as K\n",
        "from keras.engine.topology import Layer\n",
        "from keras import initializers, regularizers, constraints\n",
        "from keras.layers.merge import _Merge\n",
        "\n",
        "\n",
        "class Attention(Layer):\n",
        "    def __init__(self, step_dim,\n",
        "                 W_regularizer=None, b_regularizer=None,\n",
        "                 W_constraint=None, b_constraint=None,\n",
        "                 bias=True, **kwargs):\n",
        "        \"\"\"\n",
        "        Keras Layer that implements an Attention mechanism for temporal data.\n",
        "        Supports Masking.\n",
        "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
        "        # Input shape\n",
        "            3D tensor with shape: `(samples, steps, features)`.\n",
        "        # Output shape\n",
        "            2D tensor with shape: `(samples, features)`.\n",
        "        :param kwargs:\n",
        "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
        "        The dimensions are inferred based on the output shape of the RNN.\n",
        "        Example:\n",
        "            model.add(LSTM(64, return_sequences=True))\n",
        "            model.add(Attention())\n",
        "        \"\"\"\n",
        "        self.supports_masking = True\n",
        "        # self.init = initializations.get('glorot_uniform')\n",
        "        self.init = initializers.get('glorot_uniform')\n",
        "\n",
        "        self.W_regularizer = regularizers.get(W_regularizer)\n",
        "        self.b_regularizer = regularizers.get(b_regularizer)\n",
        "\n",
        "        self.W_constraint = constraints.get(W_constraint)\n",
        "        self.b_constraint = constraints.get(b_constraint)\n",
        "\n",
        "        self.bias = bias\n",
        "        self.step_dim = step_dim\n",
        "        self.features_dim = 0\n",
        "        super(Attention, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 3\n",
        "\n",
        "        self.W = self.add_weight((input_shape[-1],),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_W'.format(self.name),\n",
        "                                 regularizer=self.W_regularizer,\n",
        "                                 constraint=self.W_constraint)\n",
        "        self.features_dim = input_shape[-1]\n",
        "\n",
        "        if self.bias:\n",
        "            self.b = self.add_weight((input_shape[1],),\n",
        "                                     initializer='zero',\n",
        "                                     name='{}_b'.format(self.name),\n",
        "                                     regularizer=self.b_regularizer,\n",
        "                                     constraint=self.b_constraint)\n",
        "        else:\n",
        "            self.b = None\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def compute_mask(self, input, input_mask=None):\n",
        "        # do not pass the mask to the next layers\n",
        "        return None\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        input_shape = K.int_shape(x)\n",
        "\n",
        "        features_dim = self.features_dim\n",
        "        # step_dim = self.step_dim\n",
        "        step_dim = input_shape[1]\n",
        "\n",
        "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
        "\n",
        "        if self.bias:\n",
        "            eij += self.b[:input_shape[1]]\n",
        "\n",
        "        eij = K.tanh(eij)\n",
        "\n",
        "        a = K.exp(eij)\n",
        "\n",
        "        # apply mask after the exp. will be re-normalized next\n",
        "        if mask is not None:\n",
        "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
        "            a *= K.cast(mask, K.floatx())\n",
        "\n",
        "        # in some cases especially in the early stages of training the sum may be almost zero\n",
        "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
        "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
        "\n",
        "        a = K.expand_dims(a)\n",
        "        weighted_input = x * a\n",
        "    \t# print weigthted_input.shape\n",
        "        return K.sum(weighted_input, axis=1)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        # return input_shape[0], input_shape[-1]\n",
        "    \treturn input_shape[0], self.features_dim\n",
        "# end Attention\n",
        "\n",
        "\n",
        "class JoinAttention(_Merge):\n",
        "    def __init__(self, step_dim, hid_size,\n",
        "                 W_regularizer=None, b_regularizer=None,\n",
        "                 W_constraint=None, b_constraint=None,\n",
        "                 bias=True, **kwargs):\n",
        "        \"\"\"\n",
        "        Keras Layer that implements an Attention mechanism according to other vector.\n",
        "        Supports Masking.\n",
        "        # Input shape, list of\n",
        "            2D tensor with shape: `(samples, features_1)`.\n",
        "            3D tensor with shape: `(samples, steps, features_2)`.\n",
        "        # Output shape\n",
        "            2D tensor with shape: `(samples, features)`.\n",
        "        :param kwargs:\n",
        "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
        "        The dimensions are inferred based on the output shape of the RNN.\n",
        "        Example:\n",
        "            en = LSTM(64, return_sequences=False)(input)\n",
        "            de = LSTM(64, return_sequences=True)(input2)\n",
        "            output = JoinAttention(64, 20)([en, de])\n",
        "        \"\"\"\n",
        "        self.supports_masking = True\n",
        "        # self.init = initializations.get('glorot_uniform')\n",
        "        self.init = initializers.get('glorot_uniform')\n",
        "\n",
        "        self.W_regularizer = regularizers.get(W_regularizer)\n",
        "        self.b_regularizer = regularizers.get(b_regularizer)\n",
        "\n",
        "        self.W_constraint = constraints.get(W_constraint)\n",
        "        self.b_constraint = constraints.get(b_constraint)\n",
        "\n",
        "        self.bias = bias\n",
        "        self.step_dim = step_dim\n",
        "        self.hid_size = hid_size\n",
        "        super(JoinAttention, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        if not isinstance(input_shape, list):\n",
        "            raise ValueError('A merge layer [JoinAttention] should be called '\n",
        "                             'on a list of inputs.')\n",
        "        if len(input_shape) != 2:\n",
        "            raise ValueError('A merge layer [JoinAttention] should be called '\n",
        "                             'on a list of 2 inputs. '\n",
        "                             'Got ' + str(len(input_shape)) + ' inputs.')\n",
        "        if len(input_shape[0]) != 2 or len(input_shape[1]) != 3:\n",
        "            raise ValueError('A merge layer [JoinAttention] should be called '\n",
        "                             'on a list of 2 inputs with first ndim 2 and second one ndim 3. '\n",
        "                             'Got ' + str(len(input_shape)) + ' inputs.')\n",
        "\n",
        "        self.W_en1 = self.add_weight((input_shape[0][-1], self.hid_size),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_W0'.format(self.name),\n",
        "                                 regularizer=self.W_regularizer,\n",
        "                                 constraint=self.W_constraint)\n",
        "        self.W_en2 = self.add_weight((input_shape[1][-1], self.hid_size),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_W1'.format(self.name),\n",
        "                                 regularizer=self.W_regularizer,\n",
        "                                 constraint=self.W_constraint)\n",
        "        self.W_de = self.add_weight((self.hid_size,),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_W2'.format(self.name),\n",
        "                                 regularizer=self.W_regularizer,\n",
        "                                 constraint=self.W_constraint)\n",
        "\n",
        "        if self.bias:\n",
        "            self.b_en1 = self.add_weight((self.hid_size,),\n",
        "                                     initializer='zero',\n",
        "                                     name='{}_b0'.format(self.name),\n",
        "                                     regularizer=self.b_regularizer,\n",
        "                                     constraint=self.b_constraint)\n",
        "            self.b_en2 = self.add_weight((self.hid_size,),\n",
        "                                     initializer='zero',\n",
        "                                     name='{}_b1'.format(self.name),\n",
        "                                     regularizer=self.b_regularizer,\n",
        "                                     constraint=self.b_constraint)\n",
        "            self.b_de = self.add_weight((input_shape[1][1],),\n",
        "                                     initializer='zero',\n",
        "                                     name='{}_b2'.format(self.name),\n",
        "                                     regularizer=self.b_regularizer,\n",
        "                                     constraint=self.b_constraint)\n",
        "        else:\n",
        "            self.b_en1 = None\n",
        "            self.b_en2 = None\n",
        "            self.b_de = None\n",
        "\n",
        "        self._reshape_required = False\n",
        "        self.built = True\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[1][0], input_shape[1][-1]\n",
        "\n",
        "    def compute_mask(self, input, input_mask=None):\n",
        "        # do not pass the mask to the next layers\n",
        "        return None\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        en = inputs[0]\n",
        "        de = inputs[1]\n",
        "        de_shape = K.int_shape(de)\n",
        "        step_dim = de_shape[1]\n",
        "\n",
        "        hid_en = K.dot(en, self.W_en1)\n",
        "        hid_de = K.dot(de, self.W_en2)\n",
        "        if self.bias:\n",
        "            hid_en += self.b_en1\n",
        "            hid_de += self.b_en2\n",
        "        hid = K.tanh(K.expand_dims(hid_en, axis=1) + hid_de)\n",
        "        eij = K.reshape(K.dot(hid, K.reshape(self.W_de, (self.hid_size, 1))), (-1, step_dim))\n",
        "        if self.bias:\n",
        "            eij += self.b_de[:step_dim]\n",
        "\n",
        "        a = K.exp(eij - K.max(eij, axis=-1, keepdims=True))\n",
        "\n",
        "        # apply mask after the exp. will be re-normalized next\n",
        "        if mask is not None:\n",
        "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
        "            a *= K.cast(mask[1], K.floatx())\n",
        "\n",
        "        # in some cases especially in the early stages of training the sum may be almost zero\n",
        "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
        "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
        "\n",
        "        a = K.expand_dims(a)\n",
        "        weighted_input = de * a\n",
        "        return K.sum(weighted_input, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NhXAvrxJJiQm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Model"
      ]
    },
    {
      "metadata": {
        "id": "7MNLzOH1mwuw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras import Model\n",
        "from keras.layers import *\n",
        "from keras.preprocessing import text,sequence\n",
        "\n",
        "class Model_new():\n",
        "    def model(self, embeddings_matrix, maxlen_cl,maxlen_sen, word_index):\n",
        "        claim = Input(shape=(maxlen_cl,),name='claim')\n",
        "        sentence=Input(shape=(maxlen_sen,),name='sentence')\n",
        "        attention_s=Attention(maxlen_sen)\n",
        "        attention_c=Attention(maxlen_cl)\n",
        "        encode_c = CuDNNGRU(64, return_sequences=True)\n",
        "        encode_s= Bidirectional(CuDNNGRU(64, return_sequences=True))\n",
        "        encode2_s = Bidirectional(CuDNNGRU(64, return_sequences=True))\n",
        "        x_4_s = Embedding(len(word_index) + 1,\n",
        "                        embeddings_matrix.shape[1],\n",
        "                        weights=[embeddings_matrix],\n",
        "                        input_length=maxlen_sen,\n",
        "                        trainable=True)(sentence)\n",
        "        x_4_c = Embedding(len(word_index) + 1,\n",
        "                embeddings_matrix.shape[1],\n",
        "                weights=[embeddings_matrix],\n",
        "                input_length=maxlen_cl,\n",
        "                trainable=True)(claim)\n",
        "        x_3_s = SpatialDropout1D(0.2)(x_4_s)\n",
        "        x_3_s = encode_s(x_3_s)\n",
        "        x_3_s = Dropout(0.2)(x_3_s)\n",
        "        x_3_s = encode2_s(x_3_s)\n",
        "        x_3_s = Dropout(0.2)(x_3_s)\n",
        "        avg_pool_3_s = GlobalAveragePooling1D()(x_3_s)\n",
        "        max_pool_3_s = GlobalMaxPooling1D()(x_3_s)\n",
        "        attention_3_s=attention_s(x_3_s)\n",
        "        \n",
        "        x_3_c = SpatialDropout1D(0.2)(x_4_c)\n",
        "        x_3_c = encode_c(x_3_c)\n",
        "        x_3_c = Dropout(0.2)(x_3_c)\n",
        "        avg_pool_3_c = GlobalAveragePooling1D()(x_3_c)\n",
        "        max_pool_3_c = GlobalMaxPooling1D()(x_3_c)\n",
        "        attention_3_c=attention_c(x_3_c)\n",
        "        \n",
        "        x = keras.layers.concatenate([avg_pool_3_s, max_pool_3_s,attention_3_s,avg_pool_3_c, max_pool_3_c,attention_3_c], name=\"fc\")\n",
        "        main_output = Dense(1, activation=\"sigmoid\",name='main_output')(x)\n",
        "        \n",
        "        adam = keras.optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-08,amsgrad=True)\n",
        "        rmsprop = keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-06)\n",
        "        model = Model(inputs=[claim,sentence], outputs=main_output)\n",
        "        model.compile(\n",
        "            loss='binary_crossentropy',\n",
        "            metrics = ['accuracy'],\n",
        "            optimizer=adam)\n",
        "        return model\n",
        "      \n",
        "from keras.callbacks import ModelCheckpoint, Callback\n",
        "from sklearn.metrics import f1_score, recall_score, precision_score\n",
        "\n",
        "def getClassification(prob):\n",
        "    if prob<0.5:\n",
        "        return 0\n",
        "    else :\n",
        "        return 1\n",
        "\n",
        "class Metrics(Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.val_f1s = []\n",
        "        self.val_recalls = []\n",
        "        self.val_precisions = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        print('epoch end')\n",
        "        print(len(self.validation_data))\n",
        "        val_predict = list(map(getClassification, self.model.predict([self.validation_data[0],self.validation_data[1]])))\n",
        "        val_targ = list(self.validation_data[2])\n",
        "        _val_f1 = f1_score(val_targ, val_predict, average=\"macro\")\n",
        "        _val_recall = recall_score(val_targ, val_predict, average=\"macro\")\n",
        "        _val_precision = precision_score(val_targ, val_predict, average=\"macro\")\n",
        "        self.val_f1s.append(_val_f1)\n",
        "        self.val_recalls.append(_val_recall)\n",
        "        self.val_precisions.append(_val_precision)\n",
        "        print(_val_f1, _val_precision, _val_recall)\n",
        "        print(\"max f1\")\n",
        "        print(max(self.val_f1s))\n",
        "        return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aXqdzv1cs1sl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Train and Validation"
      ]
    },
    {
      "metadata": {
        "id": "mC6I8eYcmZYu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import plot_model\n",
        "import pandas as pd\n",
        "def sentence_clean(text):\n",
        "  text=text.replace('\\t','')\n",
        "  return text\n",
        "\n",
        "data=pd.read_csv('/drive/My Drive/IRDM/task6/train_task6.csv')\n",
        "model_dir='/drive/My Drive/IRDM/task6_model/'\n",
        "data['sentence'] = data.apply(lambda r: sentence_clean(r['sentence']), axis=1)\n",
        "x_train_c=data['claim'].values\n",
        "x_train_s=data['sentence'].values\n",
        "y_train=data['label'].values\n",
        "\n",
        "word2vec_model_path = '/drive/My Drive/IRDM/word2vec/glove.6B.50d.txt' \n",
        "word2vec_model = KeyedVectors.load_word2vec_format(word2vec_model_path, binary=False,unicode_errors='ignore',encoding='utf8')\n",
        "word2vec_model.init_sims(replace=True)\n",
        "\n",
        "tokenizer = text.Tokenizer(num_words=None)\n",
        "to_fit=np.concatenate((data['claim'].values,data['sentence'].values),axis=0)\n",
        "\n",
        "tokenizer.fit_on_texts(to_fit)\n",
        "word_index = tokenizer.word_index\n",
        "embeddings_matrix = np.zeros((len(word_index) + 1, word2vec_model.vector_size))\n",
        "vocab_list = [(k, word2vec_model[k]) for k, v in word2vec_model.vocab.items()]\n",
        "for word, i in word_index.items():\n",
        "    if word in word2vec_model:\n",
        "        embedding_vector = word2vec_model[word]\n",
        "        #print(embedding_vector)\n",
        "    else:\n",
        "        embedding_vector = None\n",
        "    if embedding_vector is not None:\n",
        "        embeddings_matrix[i] = embedding_vector\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Conwls8xs6qN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "maxlen_s=500\n",
        "maxlen_c=50\n",
        "model = Model_new().model(embeddings_matrix, maxlen_c,maxlen_s, word_index)\n",
        "\n",
        "file_path = model_dir + \"model_new_{epoch:02d}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(file_path, verbose=1, save_weights_only=True)\n",
        "metrics = Metrics()\n",
        "callbacks_list = [checkpoint, metrics]\n",
        "x_train_c = tokenizer.texts_to_sequences(x_train_c)\n",
        "x_train_c = sequence.pad_sequences(x_train_c, maxlen=maxlen_c)\n",
        "x_train_s = tokenizer.texts_to_sequences(x_train_s)\n",
        "x_train_s = sequence.pad_sequences(x_train_s, maxlen=maxlen_s)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QfoPJ9lKtNNk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "history = model.fit(x={'claim':x_train_c,'sentence':x_train_s},y={'main_output':y_train},validation_split=0.2, epochs=15,callbacks=callbacks_list, verbose=1,batch_size=512)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XT290UortVbk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data=pd.read_csv('/drive/My Drive/IRDM/task6/test_task6.csv')\n",
        "\n",
        "data['sentence'] = data.apply(lambda r: sentence_clean(r['sentence']), axis=1)\n",
        "x_test_c=data['claim'].values\n",
        "x_test_s=data['sentence'].values\n",
        "y_test=data['label'].values\n",
        "\n",
        "\n",
        "maxlen_s=500\n",
        "maxlen_c=50\n",
        "\n",
        "callbacks_list = [checkpoint, metrics]\n",
        "x_test_c = tokenizer.texts_to_sequences(x_test_c)\n",
        "x_test_c = sequence.pad_sequences(x_test_c, maxlen=maxlen_c)\n",
        "x_test_s = tokenizer.texts_to_sequences(x_test_s)\n",
        "x_test_s = sequence.pad_sequences(x_test_s, maxlen=maxlen_s)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d3D34eHTtZIw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "best=Model_new().model(embeddings_matrix, maxlen_c,maxlen_s, word_index)\n",
        "best.load_weights('/drive/My Drive/IRDM/task8/model/model_new_09.hdf5')\n",
        "print(model_best.metrics_names)\n",
        "\n",
        "final = model_best.evaluate(x={'claim':x_test_c,'sentence':x_test_s},\n",
        "            y={'main_output': y_test},\n",
        "            batch_size=10, verbose=1)\n",
        "print(final)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rhFLKYXaNc-v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plot_model(model, to_file='/drive/My Drive/IRDM/model_task8.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "so36nNZdjqt_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    }
  ]
}